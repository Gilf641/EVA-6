# -*- coding: utf-8 -*-
"""EVA6-S3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SmU-_HVOF6eYhnFCPQJh-WueoB18H6hM

### **Check the Alloted Device Specs**
"""

!nvidia-smi

"""

###**Mount GDrive to Colab**"""

from google.colab import drive
drive.mount('/content/gdrive')

"""###**Load required modules**"""

# Commented out IPython magic to ensure Python compatibility.
from __future__ import print_function 
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
from torch.utils.data import Dataset
import random
from torchsummary import summary
import pandas as pd
from PIL import Image
import csv
import gzip
import numpy as np
import os
from torch.utils.tensorboard import SummaryWriter
import torchvision
# %load_ext tensorboard

"""###**Set Device and other Params**"""

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
GPU_BATCH_SIZE = 128
CPU_BATCH_SIZE = 64
NUM_WORKERS = 2
LR = 0.01

# Dataset related paths
dataset_path = '/content/gdrive/MyDrive/EVA6/MNIST/Dataset/'
train_data_path, test_data_path = 'train-images-idx3-ubyte.gz', 't10k-images-idx3-ubyte.gz'
train_labels_path, test_labels_path = 'train-labels-idx1-ubyte.gz', 't10k-labels-idx1-ubyte.gz'

"""###**Functions to Get Images and Labels**"""

def getimages(path: str): 
  """
  Opens gz dataset file and returns images
  """
  if path.endswith('-idx3-ubyte.gz'):
    with gzip.open(path) as dataset:
      magic_num = int.from_bytes(dataset.read(4), 'big')
      image_count = int.from_bytes(dataset.read(4), 'big')
      row_count = int.from_bytes(dataset.read(4), 'big')
      column_count = int.from_bytes(dataset.read(4), 'big')
      images = np.frombuffer(dataset.read(), dtype=np.uint8).reshape(image_count, 1, row_count, column_count).astype(np.float32)
      print('Images Shape: ', images.shape)
      return images



def getlabels(path: str):
  """
  Opens gz dataset file and returns labels
  """
  if path.endswith('-idx1-ubyte.gz'):
    with gzip.open(path) as dataset:
      magic_number = int.from_bytes(dataset.read(4),'big')
      label_count=int.from_bytes(dataset.read(4), 'big')
      labels = np.frombuffer(dataset.read(), dtype=np.uint8).astype(np.longlong)
      print('Labels Shape: ', labels.shape)
      return labels

"""###**Data & Transformations**




"""

train_data = getimages(os.path.join(dataset_path, train_data_path))
train_labels = getlabels(os.path.join(dataset_path, train_labels_path))

test_data = getimages(os.path.join(dataset_path, test_data_path))
test_labels = getlabels(os.path.join(dataset_path, test_labels_path))

train_transforms=transforms.Compose([
                                     transforms.ToTensor(),
                                     transforms.Normalize((0.1307,), (0.3081,))
                                    ])

test_transforms=transforms.Compose([
                                    transforms.ToTensor(),
                                    transforms.Normalize((0.1307,),(0.3081,))
                                    ])

"""###**Custom Dataset(MNIST + Random Number)**"""

class RandomMNIST(Dataset):
  """
  Custom Dataset Class with 2 I/P & 2 O/P
  """
  def __init__(self, data, labels):
    self.data = data
    self.random_num = np.ones(60000)
    self.labels = labels
    self.sum = np.ones(60000)

  def __len__(self):
    return len(self.data)

  def __getitem__(self, index):
    self.random_num[index] = np.random.randint(low=0, high=9,size=(1,))
    self.sum[index]=int(self.random_num[index]) + self.labels[index]

    # self.data[index] = np.expand_dims(self.data[index], axis=0)
    return self.data[index], int(self.random_num[index]), self.labels[index], self.sum[index]

"""###**Get Data'Loader'**"""

def Data_To_Dataloader(trainset,testset,seed=1,batch_size=GPU_BATCH_SIZE, num_workers=NUM_WORKERS,pin_memory=True):
	"""
	Converts DataSet Object to DataLoader
	"""
	SEED = 1
	cuda = torch.cuda.is_available()
	torch.manual_seed(SEED)

	if cuda:
			torch.cuda.manual_seed(SEED)

	dataloader_args = dict(shuffle=True, batch_size=GPU_BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=pin_memory) if cuda else dict(shuffle=True, batch_size=CPU_BATCH_SIZE)
	trainloader = torch.utils.data.DataLoader(trainset, **dataloader_args)
	testloader = torch.utils.data.DataLoader(testset, **dataloader_args)
	return  trainloader, testloader


train = RandomMNIST(train_data, train_labels)
test = RandomMNIST(test_data, test_labels)
trainloader, testloader = Data_To_Dataloader(train, test)

"""###**Finally the NeuralNet Class**"""

class Net(nn.Module):
  """
  Simple NN Class which takes two inputs and returns two outputs
  """
  def __init__(self):
    super(Net, self).__init__() 

    self.conv1 = nn.Sequential(
        nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3,3), padding=0, bias=False),
        nn.ReLU(), 
        nn.BatchNorm2d(32)
    )
    self.conv2 = nn.Sequential(
        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(5,5), padding=0, bias=False),
        nn.ReLU(), 
        nn.BatchNorm2d(64)
    )
    self.conv3 = nn.Sequential(
        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3,3), padding=0, bias=False),
        nn.ReLU(), 
        nn.BatchNorm2d(128)
    )
    self.conv4 = nn.Sequential(
        nn.Conv2d(in_channels=128, out_channels=32, kernel_size=(1,1), padding=0, bias=False),
        nn.ReLU(), 
        nn.BatchNorm2d(32)
    )
    self.conv5 = nn.Sequential(
        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3), padding=1, bias=False)
    )
    self.conv6 = nn.Sequential(
        nn.Conv2d(in_channels=64, out_channels=32, kernel_size=(3,3), padding=0, bias=False)
    )
    self.conv7 = nn.Sequential(
        nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(1,1), padding=0, bias=False)
    )

    self.gap=nn.AdaptiveAvgPool2d(1)
    # self.pred_list=[]


  def forward(self, input, rand_num):
    x, num = input, rand_num
    
    x = self.conv1(input)
    x = self.conv2(x)
    x = self.conv3(x)
    maxpool_1 = F.max_pool2d(x, kernel_size=2, stride=2)
    x = self.conv4(maxpool_1)
    x=self.conv5(x)
    x=self.conv6(x)
    x=self.conv7(x)
    x=self.gap(x)
    x=x.view(-1,10)

    max_arg_index=torch.argmax(F.log_softmax(x,dim=-1), dim=1)
    labels=[i for i in range(0,10)]
    final_pred=[labels[i] for i in max_arg_index]    
    return F.log_softmax(x, dim=-1), num

"""###**Train & Test Network**"""

net = Net()
model=net.to(torch.device("cuda"))

# set optimizer to Adam 
optimizer=optim.Adam(model.parameters(), lr=LR)

# set loss func to CrossEntropy
criterion = nn.CrossEntropyLoss()

# Creating a tb instance of SummaryWriter
tb = SummaryWriter()

count=0

def correct_pred(pred,labels):
  """
  Returns Sum of Correct Predictions
  """
  return pred.argmax(dim=1).eq(labels).sum().item()

# Training & Testing Loop
for e in range(10):
  train_loss, train_loss_item,   train_correct=0,0, 0
  test_loss, test_correct=0,0
  print(f'Epoch:{e}')

  # running model on train data
  for batch in trainloader:
    count+=1
    images, rand_num, labels, sum = batch

    images, rand_num, labels, sum = images.to(device), rand_num.to(device), labels.to(device), sum.to(device)
    preds, n = model(images, rand_num)
    loss=criterion(preds, labels)
    
    optimizer.zero_grad()
    loss.backward() # calc grad through backprop
    optimizer.step() # update network weights

    # update loss and accuracy
    train_loss += loss
    train_loss_item += loss.item()
    train_correct += correct_pred(preds, labels)
    
    
  # running model on test data
  with torch.no_grad():
    for batch in testloader:
      images, rand_num, labels, sum = batch
      images, rand_num, labels, sum = images.to(device), rand_num.to(device), labels.to(device), sum.to(device)
      preds, num = model(images, rand_num)
      loss=criterion(preds, labels)
      test_loss += loss.item()
      test_correct += correct_pred(preds, labels)


  train_acc = train_correct/(len(trainloader)*128)*100
  test_acc = test_correct/(len(testloader)*128)*100
  print(f'TrainSet: [Accuracy={train_acc}, Loss={train_loss}]')
  print(f'TestSet: [Accuracy:{test_acc}, Loss:{test_loss}]\n')
 
  # tensorboard integration
  tb.add_scalar('Train Loss', train_loss, e )
  tb.add_scalar('Train Accuracy', train_acc, e )
  tb.add_scalar('Test Loss', test_loss, e )
  tb.add_scalar('Test Accuracy', test_acc, e )
  
  
tb.close()

"""###**TensorBoard Dashboard**"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir /content/runs

